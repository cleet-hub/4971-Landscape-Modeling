{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Landscape Modeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleet-hub/4971-Landscape-Modeling/blob/main/Landscape_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorials\n",
        "\n",
        "[Slides.Introduction to Research Computing on the Great Lakes cluster](https://docs.google.com/presentation/d/1a9oFwk5pMy-t-T-01FkwVJdhe_1mrEqYTxUSkgIcVVo/edit?usp=sharing)\n",
        "\n",
        "[Machine learning in great lakes tutorial](https://www.mivideo.it.umich.edu/playlist/dedicated/1_fgzsrgd0/1_256uvpp8) (found this more helpful in accessing data and running pyspark in the cluster) and here's the [github repository](https://github.com/cleet-hub/workshops).\n",
        "\n",
        "[List of common commands that can be used in the Great Lakes HPC cluster](https://arc.umich.edu/wp-content/uploads/sites/4/2020/05/Great-Lakes-Cheat-Sheet.pdf)\n",
        "\n",
        "NEEDS TO BE UPDATED: How to process prepare and apply pattern mining to twitter data. Globus file transfer may not be needed. Here's the read me file from the twitter dataset set below.  \n",
        "\n",
        "# About this Data\n",
        "This data is collected from the Twitter Developer Stream. It contains a 1% random sample of all public tweets.\n",
        "\n",
        "The data is collected 24/7 (as long as the stream from Twitter is available), and a new file is created at 12:00 AM Eastern and contains all the tweets collected during that 24-hour period.\n",
        "\n",
        "See the following URL for an up-to-date listing of all the files, their sizes, the date collected, and the total number of tweets for each file: http://mads-twitter-ingest.miserver.it.umich.edu/tweetCounts.txt. For a visualization of the dataset size, see http://mads-twitter-ingest.miserver.it.umich.edu/tweetCounts.txt.\n",
        "\n",
        "\n",
        "## Usage Policy\n",
        "Approved users must be added to the mads-twitter-user group in order to access the data. The data is accessible on the Great Lakes cluster, the Cavium-ThunderX cluster, and via Globus.\n",
        "\n",
        "* By using this data, you agree to the following terms.\n",
        "\n",
        "* The data is for analysis in the course only and will not be used outside of the course\n",
        "* Data should be processed on University of Michigan systems and additional copies of the data should not be made\n",
        "* Data will be deleted at the conclusion of the course \n",
        "* Users of the data are bound by the Twitter Developer Policy (https://developer.twitter.com/en/developer-terms/policy ) and are responsible for understanding and adhering to them \n",
        "\n",
        "## Data Location\n",
        "The data is located in Turbo storage at the following path: /nfs/turbo/mads-twitter/data\n",
        "\n",
        "## File Naming Convention\n",
        "Each file is named based on the date it was collected, using the following format: tweets.2021-10-26.bz2. Note that since this data is streamed live 24/7, there are occasions when the stream must be interrupted for maintenance and updates. When this occurs, a new file is generated for that day, and named part2, part3, and so on, such as: tweets.2021-10-26.part2.bz2. Therefore, in cases such as these, the user should load all the files with the date 2021-10-26 in order to load all data for that day.\n",
        "\n",
        "\n",
        "## File Format\n",
        "The tweet information received from twitter is in Twitter's JSON-formatted Tweet Object. More information on the Tweet object can be found here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
        "\n",
        "Note that not all fields are present in all tweets, so the user must account for this in their code.\n",
        "\n",
        "The files are stored in a BZip2 compressed file containing several lines of JSON, separated by a newline. Each line represents a single tweet in the above mentioned JSON Tweet Object format.\n",
        "\n",
        "A common way to process this data efficiently is by using PySpark (one line of code to load the data into a dataframe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaXxsYTX6Vm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References \n",
        "\n",
        "[Notes on landscape modeling](https://docs.google.com/document/d/1oR93DLHAbPRBDou7AXFk90IA7LAPDqY4/edit?usp=sharing&ouid=108335088878303381651&rtpof=true&sd=true)\n"
      ],
      "metadata": {
        "id": "SVjocJ7A7py3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing "
      ],
      "metadata": {
        "id": "m-ZsZAbVAycV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q-Zbbb776CQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is taken nearly verbatim from the [decahose repository](https://github.com/caocscar/twitter-decahose-pyspark) on pyspark for tweet objects. It creates a parquet file (?) from a DataFrameReader (pyspark SQL object) (lines 11-14), while selecting the relevant keys (line 12)."
      ],
      "metadata": {
        "id": "P_ZsPvbrGnIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "import os\n",
        "\n",
        "conf = SparkConf().setAppName('Tweets_sample0')\n",
        "sc = SparkContext(conf=conf)\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "wdir = 'home/nfs/turbo/mads-twitter/data'\n",
        "file = 'tweets.2021-08-23.bz2'\n",
        "df = sqlContext.read.json(os.path.join(wdir,file))\n",
        "six = df.select('created_at','user.name','user.screen_name','text','coordinates','place')\n",
        "folder = 'twitterExtract'\n",
        "six.write.mode('overwrite').parquet(folder)"
      ],
      "metadata": {
        "id": "y3eQSSiLzNj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two codes below are a play on the decahose tutorial and machine learning tutorial. Both the batch script and accompanying pyspark code are listed, respectively. The beginning code to call relevant pyspark modules in the .py code is excluded below. The first couple simply selects only the text and saves its display in text format. The second couple takes 3 samples and filters for only tweets containing 'pond' or 'ponds' (lines 5, 12, & 19). It then only selects the texts from the tweet objects and saves the display in text format. "
      ],
      "metadata": {
        "id": "0iq5KNQIIaCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tweets_test2.sh"
      ],
      "metadata": {
        "id": "NpSHtT1gJTNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "#Indicate the account to charge the job to\n",
        "#SBATCH --account=marklin0\n",
        "\n",
        "#Indicate a name to give the job. This can be anything, it's just a way to be able to easily track different jobs\n",
        "#SBATCH --job-name=tweets_test2\n",
        "\n",
        "#Indicate where to send information about the job\n",
        "#SBATCH --mail-user=cleet@umich.edu\n",
        "\n",
        "#Indicate how often you want info about the job. In this case, you will receive an email when the job has ended\n",
        "#SBATCH --mail-type=END\n",
        "#SBATCH --mail-type=FAIL\n",
        "\n",
        "#Indicate how many nodes to run the job on\n",
        "#SBATCH --nodes=1\n",
        "\n",
        "#Indicate how many tasks to run on each node\n",
        "#SBATCH --ntasks-per-node=1\n",
        "\n",
        "#Indicate how many cpus to use per task\n",
        "#SBATCH --cpus-per-task=1\n",
        "\n",
        "#Indicate how much memory to use per cpu\n",
        "#SBATCH --mem-per-cpu=1000m\n",
        "\n",
        "#Indicate how long to run the job for\n",
        "#SBATCH --time=20:00\n",
        "\n",
        "#Indicate which partition to run the job on. In this case, we will run on the standard partition\n",
        "#SBATCH --partition=standard\n",
        "\n",
        "#Indicate where to name output file\n",
        "#SBATCH --output=tweets_test2\n",
        "\n",
        "# The job command(s):\n",
        "echo \"Running on $SLURM_JOB_NODELIST\"\n",
        "echo \"Running in $(pwd)\"\n",
        "\n",
        "module load python3.8-anaconda/2021.05\n",
        "\n",
        "pip install pyspark\n",
        "\n",
        "python3 tweets_sample0.py "
      ],
      "metadata": {
        "id": "YTaTiAN7JtSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tweets_sample0.py"
      ],
      "metadata": {
        "id": "7h9SSnM_Kdln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wdir = '/scratch/marklin_root/marklin0/cleet/tweet_test'\n",
        "\n",
        "file = 'tweets.2021-09-30.bz2'\n",
        "df = sqlContext.read.json(os.path.join(wdir,file))\n",
        "t93021 = df.select('text')\n",
        "folder = 'tweets.2021-09-30'\n",
        "t93021.write.mode('overwrite').text(folder)"
      ],
      "metadata": {
        "id": "BGisELkyIanh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tweets_test5.sh"
      ],
      "metadata": {
        "id": "rAAi_YswNCqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "#Indicate the account to charge the job to\n",
        "#SBATCH --account=marklin0\n",
        "\n",
        "#Indicate a name to give the job. This can be anything, it's just a way to be able to easily track different jobs\n",
        "#SBATCH --job-name=tweets_test5\n",
        "\n",
        "#Indicate where to send information about the job\n",
        "#SBATCH --mail-user=cleet@umich.edu\n",
        "\n",
        "#Indicate how often you want info about the job. In this case, you will receive an email when the job has ended\n",
        "#SBATCH --mail-type=END\n",
        "#SBATCH --mail-type=FAIL\n",
        "\n",
        "#Indicate how many nodes to run the job on\n",
        "#SBATCH --nodes=1\n",
        "\n",
        "#Indicate how many tasks to run on each node\n",
        "#SBATCH --ntasks-per-node=1\n",
        "\n",
        "#Indicate how many cpus to use per task\n",
        "#SBATCH --cpus-per-task=1\n",
        "\n",
        "#Indicate how much memory to use per cpu\n",
        "#SBATCH --mem-per-cpu=1000m\n",
        "\n",
        "#Indicate how long to run the job for\n",
        "#SBATCH --time=20:00\n",
        "\n",
        "#Indicate which partition to run the job on. In this case, we will run on the standard partition\n",
        "#SBATCH --partition=standard\n",
        "\n",
        "#Indicate what to name output file\n",
        "#SBATCH --output=tweets_test5\n",
        "\n",
        "# The job command(s):\n",
        "echo \"Running on $SLURM_JOB_NODELIST\"\n",
        "echo \"Running in $(pwd)\"\n",
        "\n",
        "module load python3.8-anaconda/2021.05\n",
        "\n",
        "pip install pyspark\n",
        "\n",
        "python3 tweets_sample03.py"
      ],
      "metadata": {
        "id": "CVwxguDULxAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tweets_sample03.py"
      ],
      "metadata": {
        "id": "Gson24fBM8-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wdir = '/scratch/marklin_root/marklin0/cleet/tweet_test'\n",
        "\n",
        "file = 'tweets.2021-09-30.bz2'\n",
        "df = sqlContext.read.json(os.path.join(wdir,file))\n",
        "ponds93021 = df.filter(df['text'].contains(' pond ') | df['text'].contains(' ponds '))\n",
        "pt93021 = ponds93021.select('text')\n",
        "folder = 'pondTweets.93021'\n",
        "pt93021.write.mode('overwrite').text(folder)\n",
        "\n",
        "file = 'tweets.2021-09-15.bz2'\n",
        "df = sqlContext.read.json(os.path.join(wdir,file))\n",
        "ponds91521 = df.filter(df['text'].contains(' pond ') | df['text'].contains(' ponds '))\n",
        "pt91521 = ponds91521.select('text')\n",
        "folder = 'pondTweets.91521'\n",
        "pt91521.write.mode('overwrite').text(folder)\n",
        "\n",
        "file = 'tweets.2021-08-24.bz2'\n",
        "df = sqlContext.read.json(os.path.join(wdir,file))\n",
        "ponds82421 = df.filter(df['text'].contains(' pond ') | df['text'].contains(' ponds '))\n",
        "pt82421 = ponds82421.select('text')\n",
        "folder = 'pondTweets.82421'\n",
        "pt82421.write.mode('overwrite').text(folder)"
      ],
      "metadata": {
        "id": "axG_xZmLLwhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}